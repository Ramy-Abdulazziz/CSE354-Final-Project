{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"KWMGPce-Cu_J"},"source":["# **CSE354 HW3**\n","**Due date: 11:59 pm EST on April 21, 2023 (Friday)**\n","\n","---\n","For this assignment, we will use Google Colab, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You can use your Stony Brook (*.stonybrook.edu) account or your personal gmail account for coding and Google Drive to save your results.\n","\n","## **Google Colab Tutorial**\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","**This notebook would need you to train your model on Colab's GPU. However, the runtimes are limited. So ensure that your code works on the default CPU runtime before switching over to the GPU runtime.**\n","\n","## **Problem statement**\n","---\n","In this homework, you will be using language models to predict the sentiment of a given movie review. The dataset, which is given to you, is sampled from the [IMDB dataset of 50k movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The sentences are sampled to a smaller set to help with quicker computation on Colab. The data contains a review and an associated label for the sentiment of that review. The label can either be *positive* or *negative*. You have been given three files - train_data.csv, val_data.csv and test_data.csv. The training data will be used to fine-tune the language model, the val data will be used to select the best model while training and finally the test data will give the model's final performance on the data.\n","\n","To perform this task you will be using a pre-trained DistilBERT model. DistilBERT is a BERT based language model. Its size is 40% lesser than BERT, it has around 97% of BERT's language understanding capabilities and is 60% faster. You can read more about DistilBERT - https://arxiv.org/abs/1910.01108.\n","\n","You will be using the model by taking advantage of the libraries provided by Hugging Face (https://huggingface.co/). In order to use this library, it will need to be installed using the command in the cell below. You will be training four different DistilBERT models for this assignment.\n","\n","**Todos for the assignment:**\n","*   Fill in the # TODO(students) portions in this Colab file for this assignment.\n","*   Run the experiment code blocks and note down the colab outputs in a separate text file.\n","*   Use the aformentioned colab outputs for writing the report as per submission guideline that is described at the end of this colab file.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"om1HnMs-Gw3c"},"outputs":[],"source":["!pip install transformers"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"I6-oi7JhGjjB"},"source":["## **Imports**\n","---\n","\n","All the allowed imports have been done for you in the code block below. You do need and will not be allowed to use any more imports other than the ones done below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0N6kcCYG3tn"},"outputs":[],"source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import AdamW\n","import os\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","torch.manual_seed(42)\n","np.random.seed(42)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i8rQ5vERGwes"},"source":["## **Mounting your drive**\n","---\n","\n","I would highly recommend mounting you Google Drive while running this notebook. This drive could contain the path to your dataset and it will also be used to save your fine-tuned models. In case you choose to simply save the models on your Colab workspace, the models will cease to exist after the runtime disconnects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIrvZGP08z-1"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFyqw_sxnEh8"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xVvQyFjTm4hW"},"outputs":[],"source":["#Set the path of the folder where your colab file and data exist in Google Drive in the ------ porition\n","\n","# TODO(students): start\n","%cd \"drive/MyDrive/--------\"\n","# TODO(students): end"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kihyF-BLHgou"},"source":["## **Constants in the file**\n","---\n","\n","The code block below contains a few constants.\n","\n","\n","1.   **BATCH_SIZE**: The batch size input to the models. This has been set to 16 and should not be changed. In case you encounter any CUDA - out of memory errors while training your models, this value may be reduced from 16. But please mention this in your submission report.\n","2.   **EPOCHS**: The number of epochs to train your model. This should not be changed.\n","3. **TEST_PATH**: This is the path to the test_data.csv file.\n","4. **TRAIN_PATH**: This is the path to the train_data.csv file.\n","5. **VAL_PATH**: This is the path to the val_data.csv file.\n","6. **SAVE_PATH**: This is the path to directory your model will be saved. Note: This path will be altered further down in the code by appending the name of the kind of DistilBERT model you train as per your experiments.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrivEEHR0kUq"},"outputs":[],"source":["#DO NOT CHANGE THE CONSTANTS\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","TEST_PATH = \"data/test_data.csv\"\n","TRAIN_PATH = \"data/train_data.csv\"\n","VAL_PATH = \"data/val_data.csv\"\n","SAVE_PATH = \"models/DistilBERT\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iuId22dbbSq"},"outputs":[],"source":["def load_dataset(path):\n","  dataset = pd.read_csv(path)\n","  return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQk4i6bH-LUo"},"outputs":[],"source":["train_data = load_dataset(TRAIN_PATH)\n","val_data = load_dataset(VAL_PATH)\n","test_data = load_dataset(TEST_PATH)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Kz5glgyTJRkL"},"source":["## **Problem 1 (Initialize the Model Class)**\n","---\n","\n","Here, we will setup the pre-trained DistillBert model class in order to do our binary sentiment analysis task. In the code block below, you would need to load a pre-trained DistilBERT model and it's tokenizer using Hugging Face's library. The model you would need to load is called \"distilbert-base-uncased\". It would also need to have the model hyperparameter set to *num_classes* as the output shape of the model (in this case it is going to be 2, positive and negative). Please write your code between the given TODO block.\n","\n","\n","\n","More about the model and how to load it can be read at - https://huggingface.co/distilbert-base-uncased."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmXIi5Y8fmcn"},"outputs":[],"source":["class DistillBERT():\n","\n","  def __init__(self, model_name='distilbert-base-uncased', num_classes=2):\n","    # TODO(students): start\n","    \n","    # TODO(students): end\n","\n","  def get_tokenizer_and_model(self):\n","    return self.model, self.tokenizer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RZWzKZbiKMFP"},"source":["## **Problem 2 (Initialize the Dataloader Class)**\n","---\n","Here, we will setup the dataloader class which will read data, tokenize it using the DistillBert tokenizer, converts the tokenized sentence to tensors and the labels to tensors. The code block below takes your dataset(train,validation or test) and the tokenizer you loaded in the previous block and generates the DataLoader object for it. You would need to implement a part of the tokenize_data method. This method takes the given data and generates a list of token IDs for a given review along with it's label. You would need to use the tokenizer to generated the token ids (hint:refer to tokenizer.encode_plus for more details) values for each review. **Please ensure that the maximum length of an encoded review is 512 tokens. If any input data is longer than 512 words/tokens, truncate it to first 512.** \n","\n","You would also need to convert the labels to a corresponding numerical class using the label_dict dictionary. Please write your code between the given TODO block."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yJHPbHud-nH"},"outputs":[],"source":["class DatasetLoader(Dataset):\n","\n","  def __init__(self, data, tokenizer):\n","    self.data = data\n","    self.tokenizer = tokenizer\n","\n","  def tokenize_data(self):\n","    print(\"Processing data..\")\n","    tokens = []\n","    labels = []\n","    label_dict = {'positive': 1, 'negative': 0}\n","\n","    review_list = self.data['review'].to_list()\n","    label_list = self.data['sentiment'].to_list()\n","\n","    for (review, label) in tqdm(zip(review_list, label_list), total=len(review_list)):\n","      # TODO(students): start\n","\n","      \n","      # TODO(students): end\n","    \n","    tokens = pad_sequence(tokens, batch_first=True)\n","    labels = torch.tensor(labels)\n","    dataset = TensorDataset(tokens, labels)\n","    return dataset\n","\n","  def get_data_loaders(self, batch_size=32, shuffle=True):\n","    processed_dataset = self.tokenize_data()\n","\n","    data_loader = DataLoader(\n","        processed_dataset,\n","        shuffle=shuffle,\n","        batch_size=batch_size\n","    )\n","\n","    return data_loader"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VIF3eaP3Lh0W"},"source":["## **Problem 3 (Training Function)**\n","---\n","In this problem, you will write the code that will be used to run your model class on the dataset class, both of which you have written in the previous problems.\n","\n","The class below provides method to train a given model. It takes a dictionary with the following parameters:\n","\n","\n","1.   device: The device to run the model on.\n","2.   train_data: The train_data dataframe.\n","3.   val_data: The val_data dataframe.\n","4.   batch_size: The batch_size which is input to the model.\n","5.   epochs: The number of epochs to train the model.\n","6.   training_type: The type of training that your model will be undergoing. This can take four values - 'frozen_embeddings', 'top_2_training', 'top_4_training' and 'all_training'.\n","\n","#### **Problem 3(a)**\n","\n","Your first problem here would be to implement the set_training_parameters() method. In this method you will select the layers of your model to train based on the training_type. **Note: By default the Hugging Face DistilBERT has 6 layers.**\n","\n","1. frozen_embeddings: This setting is supposed to train the DistilBERT model with embeddings that are 'frozen' i.e., not trainable. You would need to ensure that the embedding layers in your model are not trainable.\n","2. top_2_training: This setting is supposed to train just the final two layers of DistilBERT (layer 5 and layer 4). All other layers before these would need to be frozen.\n","3. top_4_training: This setting is supposed to train just the final four layers of DistilBERT (layer 5, layer 4, layer 3 and layer 2). All other layers before these would need to be frozen.\n","4. all_training: All layers of DistilBERT would need to trained.\n","\n","Please write your code between the given TODO block.\n","\n","**Note: The classifier head on top of the final DistilBERT layer would always need to be trained, please do not freeze that layer.**\n","\n","**Note: You can use model.named_parameters() and iterate over all the named parameters of the model. To set the layers to not be trainable, apply layer.requires_grad = false**\n","\n","#### **Problem 3(b)**\n","\n","Your second problem would be to implement a single training step in the given loop inside the train() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would also need to propagate the loss backwards to the model and update the given optimizer's parameters.\n","\n","Please write your code between the given TODO block.\n","\n","#### **Problem 3(c)**\n","\n","Your second problem would be to implement a single validation step in the given loop inside the eval() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n","\n","Please write your code between the given TODO block.\n","\n","**Note: Consult the pytorch demos by the TAs during class for Problem 3(b) and 3(c).** (https://colab.research.google.com/drive/1Nf_5z4_g09KqOy0km4fyG4Kj2bRcEcCK?usp=sharing) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llVpLV9kiUST"},"outputs":[],"source":["class Trainer():\n","\n","  def __init__(self, options):\n","    self.device = options['device']\n","    self.train_data = options['train_data']\n","    self.val_data = options['val_data']\n","    self.batch_size = options['batch_size']\n","    self.epochs = options['epochs']\n","    self.save_path = options['save_path']\n","    self.training_type = options['training_type']\n","    transformer = DistillBERT()\n","    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n","    self.model.to(self.device)\n","\n","  def get_performance_metrics(self, preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n","    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n","    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n","    return precision, recall, f1\n","\n","  def set_training_parameters(self):\n","    # TODO(students): start\n","\n","\n","    # TODO(students): end\n","\n","  def train(self, data_loader, optimizer):\n","    self.model.train()\n","    total_recall = 0\n","    total_precision = 0\n","    total_f1 = 0\n","    total_loss = 0\n","\n","    for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n","      self.model.zero_grad()\n","      # TODO(students): start\n","\n","\n","      # TODO(students): end\n","\n","    precision = total_precision/len(data_loader)\n","    recall = total_recall/len(data_loader)\n","    f1 = total_f1/len(data_loader)\n","    loss = total_loss/len(data_loader)\n","\n","    return precision, recall, f1, loss\n","\n","  def eval(self, data_loader):\n","    self.model.eval()\n","    total_recall = 0\n","    total_precision = 0\n","    total_f1 = 0\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","      for (reviews, labels) in tqdm(data_loader):\n","        # TODO(students): start\n","\n","\n","        # TODO(students): end\n","    \n","    precision = total_precision/len(data_loader)\n","    recall = total_recall/len(data_loader)\n","    f1 = total_f1/len(data_loader)\n","    loss = total_loss/len(data_loader)\n","\n","    return precision, recall, f1, loss\n","\n","  def save_transformer(self):\n","    self.model.save_pretrained(self.save_path)\n","    self.tokenizer.save_pretrained(self.save_path)\n","\n","  def execute(self):\n","    last_best = 0\n","    train_dataset = DatasetLoader(self.train_data, self.tokenizer)\n","    train_data_loader = train_dataset.get_data_loaders(self.batch_size)\n","    val_dataset = DatasetLoader(self.val_data, self.tokenizer)\n","    val_data_loader = val_dataset.get_data_loaders(self.batch_size)\n","    optimizer = torch.optim.AdamW(self.model.parameters(), lr = 3e-5, eps = 1e-8)\n","    self.set_training_parameters()\n","    for epoch_i in range(0, self.epochs):\n","      train_precision, train_recall, train_f1, train_loss = self.train(train_data_loader, optimizer)\n","      print(f'Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_f1: {train_f1:.4f}')\n","      val_precision, val_recall, val_f1, val_loss = self.eval(val_data_loader)\n","      print(f'Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_f1: {val_f1:.4f}')\n","\n","      if val_f1 > last_best:\n","        print(\"Saving model..\")\n","        self.save_transformer()\n","        last_best = val_f1\n","        print(\"Model saved.\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Bebt5gVsPSUi"},"source":["**Notes: Run the following blocks in order to train the model and save it in your Google Drive. There will be variations due to random initializations. Most of the experiment validation and test accuracy should be between 80%-95%. Each experiment should not take more than 30 minutes to run when runtime is set to GPU.**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yt8LCF-CQRzS"},"source":["#### **Experiment 1**\n","---\n","Training your DistilBERT with frozen embeddings.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWi6MQDJn3MO"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_frozen_embeddings'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'frozen_embeddings'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mZ_PDuUAQcGb"},"source":["#### **Experiment 2**\n","---\n","Training your DistilBERT with only top 2 layers being trained. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrZS-fPSaq7g"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_top_2_training'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'top_2_training'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DLlQtrpOQhr1"},"source":["#### **Experiment 3**\n","---\n","Training your DistilBERT with only top 4 layers being trained. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMkzOXZja66z"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_top_4_training'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'top_4_training'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bMqn3uoTQkwV"},"source":["#### **Experiment 4**\n","---\n","Training your DistilBERT with all layers being trained. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yd1DUHuZa9Zz"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_all_training'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'all_training'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6JeEqQ1bQ4j3"},"source":["## **Problem 4 (Test Function)**\n","---\n","Here, you will write the code for the testing of the models that you trained in the previous code blocks. \n","\n","The class below provides method to test a given model. It takes a dictionary with the following parameters:\n","\n","1.   device: The device to run the model on.\n","2.   test_data: The test_data dataframe.\n","3.   batch_size: The batch_size which is input to the model.\n","4.   save_path: The directory of your saved model.\n","\n","You would need to implement a single test step in the given loop inside the test() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n","\n","Please write your code between the given TODO block.\n","\n","Hint: This problem is very similar to 3(c)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URDu5ew3q5ke"},"outputs":[],"source":["class Tester():\n","\n","  def __init__(self, options):\n","    self.save_path = options['save_path']\n","    self.device = options['device']\n","    self.test_data = options['test_data']\n","    self.batch_size = options['batch_size']\n","    transformer = DistillBERT(self.save_path)\n","    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n","    self.model.to(self.device)\n","\n","  def get_performance_metrics(self, preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n","    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n","    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n","    return precision, recall, f1\n","\n","  def test(self, data_loader):\n","    self.model.eval()\n","    total_recall = 0\n","    total_precision = 0\n","    total_f1 = 0\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","      for (reviews, labels) in tqdm(data_loader):\n","        # TODO(students): start\n","\n","\n","        # TODO(students): end\n","    \n","    precision = total_precision/len(data_loader)\n","    recall = total_recall/len(data_loader)\n","    f1 = total_f1/len(data_loader)\n","    loss = total_loss/len(data_loader)\n","\n","    return precision, recall, f1, loss\n","\n","  def execute(self):\n","    test_dataset = DatasetLoader(self.test_data, self.tokenizer)\n","    test_data_loader = test_dataset.get_data_loaders(self.batch_size)\n","\n","    test_precision, test_recall, test_f1, test_loss = self.test(test_data_loader)\n","\n","    print()\n","    print(f'test_loss: {test_loss:.4f} test_precision: {test_precision:.4f} test_recall: {test_recall:.4f} test_f1: {test_f1:.4f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"h1d4bih3SMTM"},"source":["**Notes: Run these blocks only after Experiment 1 to 4 are completed and the models are saved in the \"models\" folder. Copy the output blocks into another text file for report writing.**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lkeN4URqRkRD"},"source":["#### **Experiment 5**\n","---\n","Testing your DistilBERT trained with frozen embeddings.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiCG8IQl0qnb"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_frozen_embeddings'\n","tester = Tester(options)\n","tester.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VXy2Yv6sRo2F"},"source":["#### **Experiment 6**\n","---\n","Testing your DistilBERT trained with all layers frozen except the final two layers.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y13wweAicsEJ"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_top_2_training'\n","tester = Tester(options)\n","tester.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dc1CWEB-R06K"},"source":["#### **Experiment 7**\n","---\n","Testing your DistilBERT trained with all layers frozen except the final four layers.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvBdkdX4cvSU"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_top_4_training'\n","tester = Tester(options)\n","tester.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"79g8GwS4R3eB"},"source":["#### **Experiment 8**\n","---\n","Testing your DistilBERT trained with all layers trainable.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0wt64hdcyqR"},"outputs":[],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_all_training'\n","tester = Tester(options)\n","tester.execute()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AEPJSuFSz17f"},"source":["## **Results**\n","---\n","\n","Answer the following questions based on the analyses you have performed above:"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0W-YZW0Y0P2T"},"source":["### 1. Briefly explain your code implementations for each TO-DO task."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F2a8Tg4J0UpW"},"source":["TODO [STUDENT]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OLgIfurZ0XdZ"},"source":["### 2. A table containing the precision, recall and F1 scores of each DistilBERT model during validation and testing."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AlhzgwKx0c1r"},"source":["TODO [STUDENT]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j-pmDmAJ0f3I"},"source":["### 3. An analysis explaining your understanding of the impact freezing/training different layers has on the model's performance."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vIEL3iVt0hsl"},"source":["TODO [STUDENT]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p5UR_s7ySnb0"},"source":["## **Submission guidelines**\n","---\n","You would need to submit the following files:\n","\n","\n","1.   `NLP_HW3.ipynb` - This jupyter notebook. It will also work as your report, so please add description to code wherever required. Also make sure to write your analyses outcomes in the RESULTS section above.\n","2.   `gdrive_link.txt` - Should contain a wgetable to a folder that contains your four DistilBERT models. Please make sure you provide the necessary permissions.\n","\n","**Colab design credit**: TA Dhruv Verma"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
